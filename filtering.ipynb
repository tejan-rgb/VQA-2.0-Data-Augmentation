{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf554b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import json\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import itertools\n",
    "regex_punct = r\"[!\\\"#\\$%&\\'\\(\\)\\*\\+,-\\./:;<=>\\?@\\[\\\\\\]\\^_`{\\|}~]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67640734",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Project 1/lr_question_prediction_model/lr.sav', 'rb') as f:\n",
    "    lr = pickle.load(f)\n",
    "    \n",
    "with open('../Project 1/total_labels/color_label2ans.json', 'r') as file:\n",
    "    total_color_labels = json.load(file)\n",
    "    \n",
    "with open('../Project 1/total_labels/number_label2ans.json', 'r') as file:\n",
    "    total_number_labels = json.load(file)\n",
    "    \n",
    "with open('../Project 1/total_labels/length_label2ans.json', 'r') as file:\n",
    "    total_length_labels = json.load(file)\n",
    "    \n",
    "with open('../Project 1/total_labels/time_label2ans.json', 'r') as file:\n",
    "    total_time_labels = json.load(file)\n",
    "    \n",
    "with open('../Project 1/total_labels/bool_label2ans.json', 'r') as file:\n",
    "    total_bool_labels = json.load(file)\n",
    "    \n",
    "with open(\"../Project 1/VQA_training_code/cluster_data/Using Entire Model/profitable_clusters/profitable_clusters_minDiff_2.pickle\", 'rb') as f:\n",
    "    pc = pickle.load(f)\n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "st = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcba9a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_words = ['what', 'which', 'who', 'where', 'why', 'when', 'whose', 'whom', 'how']\n",
    "\n",
    "dobj_noun = [('dobj', 'NN'), ('dobj', 'NNS')]\n",
    "nsubj_noun = [('nsubj', 'NN'), ('nsubj', 'NNS')]\n",
    "nsubjpass_noun = [('nsubjpass', 'NN'), ('nsubjpass', 'NNS')]\n",
    "pobj_noun = [('pobj', 'NN'), ('pobj', 'NNS')]\n",
    "simple_noun = ['NN', 'NNS']\n",
    "\n",
    "\n",
    "def something(list_of_tokens, token_head_text):\n",
    "    \n",
    "    if list_of_tokens != []:\n",
    "        \n",
    "        for token in list_of_tokens:\n",
    "            if token.dep_ == 'pobj' and token.tag_ in ['NN', 'NNS']:\n",
    "                return True, str(token)\n",
    "            \n",
    "        children_of_children = list(itertools.chain.from_iterable([list(token.children) for token in list_of_tokens]))\n",
    "        bool_state, token = something(children_of_children, token_head_text)\n",
    "        \n",
    "        return bool_state, str(token)\n",
    "    \n",
    "    else:\n",
    "        return True, token_head_text[0]\n",
    "\n",
    "def DeterminerCase(spacy_sentence):\n",
    "\n",
    "    token_head_text = []\n",
    "    \n",
    "    for token in spacy_sentence:\n",
    "        if token.text in question_words and token.dep_ == 'det' and token.head.tag_ in ['NN', 'NNS']:\n",
    "            children_of_head_token = [child for child in token.head.children if child != token]\n",
    "            \n",
    "            token_head_text.append(token.head.text)\n",
    "\n",
    "   \n",
    "    if token_head_text != []:\n",
    "            \n",
    "        return something(children_of_head_token, token_head_text)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return False, 'No Determiner Case'\n",
    "\n",
    "\n",
    "def Noun(text):\n",
    "    \n",
    "    spacy_sentence = nlp(text)\n",
    "    \n",
    "    token_in_string_format = [str(token) for token in spacy_sentence]\n",
    "    \n",
    "    dependency = [token.dep_ for token in spacy_sentence]\n",
    "    pos = [token.tag_ for token in spacy_sentence]\n",
    "    \n",
    "    dep_and_pos_zipped = list(zip(dependency, pos))\n",
    "    \n",
    "    if DeterminerCase(spacy_sentence)[0] == True:\n",
    "        \n",
    "        return [DeterminerCase(spacy_sentence)[1]]\n",
    "    \n",
    "    \n",
    "    elif any(i in dep_and_pos_zipped for i in dobj_noun):\n",
    "        \n",
    "        texts = []\n",
    "        for j in dobj_noun:\n",
    "            if j in dep_and_pos_zipped:\n",
    "                indices = [ind for ind, ele in enumerate(dep_and_pos_zipped) if j == ele]\n",
    "                for index in indices:\n",
    "                    text = token_in_string_format[index]\n",
    "                    texts.append(text)\n",
    "                \n",
    "        return texts\n",
    "    \n",
    "\n",
    "    elif any(i in dep_and_pos_zipped for i in nsubj_noun):\n",
    "        \n",
    "        texts = []\n",
    "        for j in nsubj_noun:\n",
    "            if j in dep_and_pos_zipped:\n",
    "                indices = [ind for ind, ele in enumerate(dep_and_pos_zipped) if j == ele]\n",
    "                for index in indices:\n",
    "                    text = token_in_string_format[index]\n",
    "                    texts.append(text)\n",
    "                \n",
    "        return texts\n",
    "    \n",
    "    \n",
    "    elif any(i in dep_and_pos_zipped for i in nsubjpass_noun):\n",
    "        \n",
    "        texts = []\n",
    "        for j in nsubjpass_noun:\n",
    "            if j in dep_and_pos_zipped:\n",
    "                indices = [ind for ind, ele in enumerate(dep_and_pos_zipped) if j == ele]\n",
    "                for index in indices:\n",
    "                    text = token_in_string_format[index]\n",
    "                    texts.append(text)\n",
    "                \n",
    "        return texts\n",
    "\n",
    "\n",
    "    elif any(i in dep_and_pos_zipped for i in pobj_noun):\n",
    "        \n",
    "        texts = []\n",
    "        for j in pobj_noun:\n",
    "            if j in dep_and_pos_zipped:\n",
    "                indices = [ind for ind, ele in enumerate(dep_and_pos_zipped) if j == ele]\n",
    "                for index in indices:\n",
    "                    text = token_in_string_format[index]\n",
    "                    texts.append(text)\n",
    "                \n",
    "        return texts\n",
    "    \n",
    "    \n",
    "    elif any(i in pos for i in simple_noun):\n",
    "        \n",
    "        texts = []\n",
    "        for j in simple_noun:\n",
    "            if j in pos:\n",
    "                indices = [ind for ind, ele in enumerate(pos) if j == ele]\n",
    "                for index in indices:\n",
    "                    text = token_in_string_format[index]\n",
    "                    texts.append(text)\n",
    "                \n",
    "        return texts\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return '-'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e4b0dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vector(word):\n",
    "    \n",
    "    vector = []\n",
    "    \n",
    "    word_in_list = word.split(\" \")\n",
    "    for i in word_in_list:\n",
    "        try:\n",
    "            vec = word2vec[i]\n",
    "            vector.append(vec)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    if vector != []:\n",
    "        return sum(vector)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def MaxCosineValue(word, list_of_words):\n",
    "    \n",
    "    word_vec = Vector(word)\n",
    "    list_of_words_vec = list(map(lambda x : Vector(x), list_of_words))\n",
    "    list_of_words_vec = list(filter(lambda x: np.all(x!=0), list_of_words_vec))\n",
    "    max_cosine_value = max(list(map(lambda x: cosine_similarity([x], [word_vec]).tolist()[0][0], list_of_words_vec)))\n",
    "    \n",
    "    return max_cosine_value\n",
    "                        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daa14fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClusterSelection(question, answers, pc):\n",
    "    \n",
    "    quesAndAns_2_cluster = {}\n",
    "    \n",
    "#     print(f\"question: {question}, anwsers: {answers}\")\n",
    "    \n",
    "    nouns = Noun(question)\n",
    "#     print(f\"nouns in question: {nouns}\")\n",
    "        \n",
    "    for noun in nouns:\n",
    "        \n",
    "#         print(f\"noun in consideration: {noun}\")\n",
    "\n",
    "        noun_in_clusters = 0\n",
    "\n",
    "        for cn, cl in pc.items():\n",
    "            if noun in cn:\n",
    "                noun_in_clusters += 1\n",
    "                quesAndAns_2_cluster[(question, answers)] = quesAndAns_2_cluster.get((question, answers), []) + [cl]\n",
    "                \n",
    "#                 print(f\"'{noun}' found a cluster\")\n",
    "            \n",
    "        if noun_in_clusters == 0:\n",
    "\n",
    "#             print(f\"'{noun}' did not find any cluster\")\n",
    "\n",
    "            maxCosineValue_and_clusteredLabels_list = []\n",
    "            for clustered_nouns, clustered_labels in pc.items():\n",
    "                max_cosine_value = MaxCosineValue(noun, clustered_nouns)\n",
    "                if max_cosine_value >= 0.4:\n",
    "                    maxCosineValue_and_clusteredLabels_list.append((max_cosine_value, clustered_labels))\n",
    "\n",
    "            if maxCosineValue_and_clusteredLabels_list != []:\n",
    "                \n",
    "#                 print(f\"'{noun}' found a cluster using cosine similarity\")\n",
    "                \n",
    "                cosine_values_list, clustered_labels_list = zip(*maxCosineValue_and_clusteredLabels_list)\n",
    "                max_value = max(cosine_values_list)\n",
    "                max_cosine_value_indices = np.where(np.array(cosine_values_list) == max_value)[0].tolist()\n",
    "                \n",
    "                for i in max_cosine_value_indices:\n",
    "                    quesAndAns_2_cluster[(question, answers)] = quesAndAns_2_cluster.get((question, answers), []) + [clustered_labels_list[i]]\n",
    "               \n",
    "\n",
    "    \n",
    "    return quesAndAns_2_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "642a646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "('man',)\n",
      "('room',)\n",
      "('couch',)\n",
      "('backpack',)\n",
      "('book',)\n",
      "('chair',)\n",
      "('man',)\n",
      "('room',)\n",
      "('laptop',)\n",
      "('couch',)\n",
      "('chair',)\n",
      "('table',)\n",
      "('man',)\n",
      "('lot',)\n",
      "('man',)\n",
      "('tv',)\n",
      "{'Who is standing in the living room with a lot of toys?': {'man'}, 'Where is the man standing?': {'room'}, 'How many toys does the man in the living room have?': {'lot'}, 'What type of furniture has a backpack on it?': {'couch'}, 'What is on the arafed couch with a book on it?': {'backpack'}, 'What is on the back of the arafed couch?': {'book'}, 'A close up of a couch and what other object in the room?': {'chair'}, 'What is the name of the person in the room with a laptop?': {'man'}, 'Where is the laptop?': {'room'}, 'What is he holding in a room?': {'laptop'}, 'Along with a chair, what else is in the room?': {'couch'}, 'Along with a couch, what else is in the room?': {'chair'}, 'Along with a couch and chair, what else is in the room?': {'table'}, 'Who is standing in the living room with a lot of clutter?': {'man'}, 'How much clutter is there in the living room?': {'lot'}, 'Who is standing in the living room with a tv?': {'man'}, 'What does the man in the living room have?': {'tv'}}\n",
      "[5, 5, 5, 5, 5, 5, 5, 5]\n",
      "('man',)\n",
      "('rock',)\n",
      "('dog',)\n",
      "('man',)\n",
      "('rock',)\n",
      "('river',)\n",
      "('dog',)\n",
      "('rocks',)\n",
      "{'Who is sitting on a rock with his dog?': {'man'}, 'What is the man sitting on with his dog?': {'rock'}, 'What animal is sitting on the rock with the man?': {'dog'}, 'Who sits on a rock next to the river?': {'man'}, 'What is the man sitting on next to the river?': {'rock'}, 'What is next to the rock?': {'river'}, 'What animal is standing on rocks?': {'dog'}, 'What is the dog standing on?': {'rocks'}}\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "\n",
    "for root, dirs, filenames in os.walk(\"/home/tejan/questions/\"):\n",
    "    files.extend(filenames)\n",
    "\n",
    "\n",
    "for i, f in enumerate(files[8:10]):\n",
    "    \n",
    "    questions = pickle.load(open(f\"/home/tejan/questions/{f}\", \"rb\"))\n",
    "\n",
    "    questions_new = []\n",
    "    for q in questions.keys():\n",
    "        q = q.lower()\n",
    "        ques = re.sub(regex_punct, \"\", q, 0, re.MULTILINE)\n",
    "        questions_new.append(ques)\n",
    "\n",
    "    question_embeddings = st.encode(questions_new, batch_size=64, show_progress_bar=False, convert_to_numpy=True)\n",
    "    pred = lr.predict(question_embeddings).tolist()\n",
    "    print(pred)\n",
    "    \n",
    "    questions_after_filtering = {}\n",
    "    for pred, (question, answers) in zip(pred, list(questions.items())):\n",
    "    \n",
    "        if pred == 0:\n",
    "            \n",
    "            for ans in answers:\n",
    "                if ans in total_color_labels:\n",
    "                    questions_after_filtering[question] = {ans}\n",
    "\n",
    "        elif pred == 1:\n",
    "            \n",
    "            for ans in answers:\n",
    "                if ans in total_number_labels:\n",
    "                    questions_after_filtering[question] = {ans}\n",
    "\n",
    "\n",
    "        elif pred == 2:\n",
    "            \n",
    "            for ans in answers:\n",
    "                if ans in total_length_labels:\n",
    "                    questions_after_filtering[question] = {ans}\n",
    "\n",
    "\n",
    "        elif pred == 3:\n",
    "            \n",
    "            for ans in answers:\n",
    "                if ans in total_time_labels:\n",
    "                    questions_after_filtering[question] = {ans}\n",
    "\n",
    "\n",
    "        elif pred == 4:\n",
    "            \n",
    "            for ans in answers:\n",
    "                if ans in total_bool_labels:\n",
    "                    questions_after_filtering[question] = {ans}\n",
    "\n",
    "    \n",
    "        if pred == 5:\n",
    "        \n",
    "            answers = tuple(answers)\n",
    "\n",
    "            quesAndAns_2_cluster = ClusterSelection(question, answers, pc)\n",
    "#             print(len(quesAndAns_2_cluster.values()))\n",
    "            clustered_labels = list(set(itertools.chain.from_iterable(list(quesAndAns_2_cluster.values())[0])))\n",
    "            for ques, ans in quesAndAns_2_cluster.keys():\n",
    "                if ans[0] in clustered_labels:\n",
    "                    print(ans)\n",
    "                    questions_after_filtering[ques] = set(ans)\n",
    "\n",
    "    print(questions_after_filtering)\n",
    "\n",
    "    pickle.dump(questions_after_filtering, open(f\"/home/tejan/questions_after_filtering/{f}\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf49c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
